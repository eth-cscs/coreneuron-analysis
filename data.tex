Up to this point the actual layout of data in memory has been treated in a somewhat abstract manner, with no detail about how it is actually implemented in \neuron.
For example, the pseudo-code examples have treated the data for each mechanism as though it was stored as a \emph{structure of arrays} (SoA), when it is actually stored as an \emph{array of scructures}.

Here we will describe the data layout as it is currently implemented. Before generalizing the storage, and suggesting optimization strategies, we want to understand the current data model, and all of the use cases that it must support.

\begin{note}
A good spot to start refactoring would be a simple serialization framework for reading and writing the state of a cell group in \neuron. This would be a good exercise in getting into the nitty-gritty of the memory layout, and could be used as input data for unit tests when trying out new approaches.
\end{note}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parallel Data Distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Each neuron cell is represented as of a tree of nodes, as illustrated in \fig{fig:tree}.
The properties of individual cells, including the number of nodes and the type, number and distribution of synapses, vary significantly.
These properties affect the computational resources required to perform a time step for a cell, e.g. cells with more synapses require more expensive \lst{state} and \lst{current} computations.
The number and type of mechanisms in a cell can be used as an \textit{a-priori estimate} its computational complexity.

To ensure load balancing, cells are organized into groups that have roughly equivalent computational overheads during circuit generation.
As a result, some cell groups have more cells than others to ensure that total computational effort required per group is balanced.
Each group of cells is stored in a separate \file{.dat} file, which are then distributed in a round-robin fashion when simulation data is loaded in \lst{nrn_setup}.

Parallelism is implemented distributing the cells, with individual cells processed serially.
There are two levels of parallelism:
\begin{enumerate}
\item
    \textbf{MPI (course-grained)}: the cell groups are distributed between MPI ranks.
\item
    \textbf{Thread (fine-grained)}: each cell group on an MPI rank is assigned to a thread.
\end{enumerate}
Threading is performed using pthreads, with explicit communication of spike information between both threads and MPI processes.
Each cell group is stored in a \lst{NrnThread} data structure (see \fig{lst:NrnThreadInfo}), which is named as such because each cell group is assinged to a thread (the name should be changed to better represent the data).

\begin{note}
The fine fine-grained parallelism could be implemented on a finer scale than it currently is, i.e. with multiple threads processing each cell group/cell. There was a CUDA implementation of Neuron that took this approach, and on the GPU this is probably necessary to expose enough parallelism. There have been some successful tests on the Xeon Phi where each thread was assigned a cell group.
\end{note}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cell Groups}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The cells in a cell group are ``flattened'' together in the \lst{NrnThread} data structure, i.e. individual cells are not stored seperately.
As an example, for the TEST2 dataset distributed with the PCP:
    \begin{itemize}
    \item
        there are around 60--70 cells per cell group.
    \item
        each cell has of the order 400--450 nodes.
    \item
        each cell group has 25,000--30,000 nodes in total.
    \end{itemize}
    The cells are packed together with all node and mechanism data for all nodes in all cells in the cell group packed into flat arrays.
        Given $n_c$ cells in the cell group with a total of $n$ nodes, the root nodes are indexed \lst{[1:n_c]}, and the rest of the nodes are indexed \lst{[n_c+1:n]}\footnote{This is evident in the loops over \lst{child_nodes=ncells+1:nnondes} and \lst{1:ncells}}.

%-----------------------------------------------------------------------------
\begin{note}
Storing the cells in the cell group as such means that all of the steps in a timestep in \fig{fig:timestepDiagram} are applied to the entire cell group, not to individual cells.
This ignores the fact that the cells can be integrated in time independently, with coordination only required when communicating spike events.
An obvious approach for introducing more fine-grained parallelism is to distriubute individual cells, not cell groups, between threads.
This approach exposes more work that can be performed in parallel, however, it has load balancing issues, because of the variation in complexity between indivdual cells.
\end{note}
%-----------------------------------------------------------------------------

The main storage requirements for a mechanism are storing the vectors relating to the linear system and the vectors that describe mechanism state.
These vectors are not allocated separately, instead a single memory buffer is allocated (\lst{NrnThread::data[]}), and the vector pointers point to sub-arrays in \lst{NrnThread::data[]}
\footnote{
Allocating memory in one large block is more memory efficient than allocating many smaller vectors with individual calls to \lst{malloc}.
}.
This process of allocating the memory is performed in the function \lst{read_phase2} in \file{nrniv/nrn_setup.cpp}.

The layout of vectors in the \lst{data[]} array is shown in \fig{}.
When the cell group is read from file, the total size of the \lst{data[]} array is determined, then the memory allocated.
The vectors that describe the linear system and the area of each node, \lst{[VEC_RHS, VEC_D, VEC_A, VEC_B, VEC_V, actual_area]}, are placed at the front of the data vector.
The state data for the mechanism then follows.

%------------------------------------------------------------------
\begin{figure}[htp!]
\input{images/dataStorage.tex}
\caption{Layout of matrix and mechanism state data for a cell group.}
\end{figure}
%------------------------------------------------------------------

\todo{are ion channels "one per node"?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Neuron DSL and \hoc Files}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Mechanisms in Neuron are described using a DSL, which is translated into C code from \hoc files.
\neuron has the C files translated from the \hoc files that used by HBPNeuron\footnote{This reduces the complexity of \neuron, decoupling \neuron from Neuron, which will make it easier modify how the mechanisms are defined.}.
The translated mechanism definitions are in \file{mech/cfiles}, with one mechanism per C source file.
Each file defines functions (like \lst{jacob}, \lst{current}, \lst{alloc}) and meta-data (such as the number of variables required to store a mechanism's state for a node in the tree).
\begin{itemize}
\item
    The mechanism data is stored in global two arrays: \lst{Memb_list memb_list[]} and \lst{Memb_func memb_func[]}.
    \begin{enumerate}
    \item
        The \lst{Memb_func} type has function pointers to the \lst{jacob}, \lst{current}, \lst{state} and other mechanism-specific functions, and other meta-data specific to the mechanism.
    \item
        The \lst{Memb_list} has a pointer to the per-node data, and a list of all the nodes that the mechanism is defined for.
    \end{enumerate}
\item
    The implementation of a mechanism in \lst{mech/cfiles} provides has a function \lst{??} that calls the \lst{register_mech()} function, which adds the mechanisms function callbacks for \lst{jacob} etc, along with meta-data into the global arrays (see \fig{lst:register_mech}).
\item
    Each thread has a list of mechanisms assigned to it, which are accessed via a linked list \lst{NrnThread::mechanisms} (see \fig{lst:NrnThreadInfo} where I have changed the name \lst{tml} to \lst{mechanisms}, to better match the pseudo code.) The linked list indexes the global arrays \lst{memb_list} and \lst{memb_func} \hilight{(why not use an array instead of a linked list?)}.
\item
    There are many opportunities to improve the interface between the runtime (solvers etc) and user-defined mechanisms.
    This model is well-suited to standard object-oriented design.
    Furthermore, much of the meta-data that is currently passed as runtime parameters could be stored as type-information that could help the compiler optimize more agressively.
\end{itemize}

\noindent
Mechanisms and their storage:
\begin{itemize}
\item
    All mechanisms are not applied to different nodes. For example, the \lst{ProbAMPANDMDA_EMS} mechanism will only be applied at a subset of the nodes in a cell.
\item
    Each mechanism has ``state'' that is stored for each node to which it is applied. This state is a set of double-precision values (e.g. a set of values describing the time evolution of a ordinary differential equation).
\item
    The number of state variables varies between mechanisms, ranging from 3 to 35 values.
\item
    Each entry in \lst{memb_list[]} stores 
    \begin{itemize}
    \item
        \lst{int nodecount}: the number of nodes to which
    \item
        \lst{int nodeindices[nodecount]}: the indexes of the nodes to which the mechanism is to be applied.
    \item
        \lst{double data[nodecount*var_per_node]}: AoS storage for the mechanism  values.
    \end{itemize}
\end{itemize}

%------------------------------------------------------------------
\filelisting[C++] {./code/storage.cpp}{cell group storge in a \texttt{NrnThread} struct}
%\label{lst:NrnThreadInfo}
\filelisting[C++] {./code/register_mech.cpp}{registering a mechanism}
%\label{lst:register_mech}
%------------------------------------------------------------------

%------------------------------------------------------------------
\begin{figure}[htp!]
\input{images/AoSvsSoA.tex}
\caption{The current AoS layout of mechanism parameters for all applicable nodes.}
\end{figure}
%------------------------------------------------------------------

\noindent
Observations
\begin{itemize}
\item
    Splitting the cells into seperate, thread-specific, data structures complicates the code. This feels like it was added at some point to facilicate threading.
\item
    Could be stripped away, to store all cells on a node/numa-region/device into a single pool.
\item
    The AoS storage is inefficient:
    \begin{itemize}
    \item
        It doesn't vectorize (see \fig{fig:papisample}).
    \item
        Poor cache/bandwidth utlization for loops (such as the \lst{jacob} update) where only one or two data values in a mechanism are touched. For each 64 byte cache line loaded, only 8 bytes of 64 are used.
    \end{itemize}
    An SoA storage would address both issues.
\item
    With SoA vectorization of most loops would still not be possible, because of the gather/scatter implicit in using the node indexes to read/write to the V and RHS vectors.
    \begin{itemize}
    \item
        Perform scatter before computing the current, then gather after.
    \end{itemize}
\end{itemize}

