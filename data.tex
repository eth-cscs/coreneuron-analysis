Up to this point the layout of data in memory has been treated in an abstract manner, with no detail about how it is actually implemented in \neuron.
For example, the pseudo-code examples have treated the data for each mechanism as though it was stored as a \emph{structure of arrays (SoA)}, when it is actually stored as an \emph{array of scructures (AoS)}.

Here we will describe the data layout as it is currently implemented. Before generalizing the storage, and suggesting optimization strategies, we want to understand the current data model, and all of the use cases that it must support.

\begin{note}
A good spot to start refactoring would be a simple serialization framework for reading and writing the state of a cell group in \neuron. This would be a good exercise in getting into the nitty-gritty of the memory layout, and could be used as input data for unit tests when trying out new approaches.
\end{note}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parallel Data Distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Each neuron cell is represented as of a tree of nodes, as illustrated in \fig{fig:tree}.
The properties of individual cells, including the number of nodes and the type, number and distribution of synapses, vary significantly.
These properties affect the computational resources required to perform a time step for a cell, e.g. cells with more synapses require more expensive \lst{state} and \lst{current} computations.
The number and type of mechanisms in a cell can be used as an \textit{a-priori} estimate of its computational complexity.

To ensure load balancing, cells are organized into groups during circuit generation, where each group has roughly equivalent computational overhead.
As a result, some cell groups have more cells than others to ensure that total computational effort required per group is balanced.
Each group of cells is stored in a separate \file{.dat} file, which are then distributed in a round-robin fashion when simulation data is loaded in \lst{nrn_setup}.

Inside the time stepping loop individual cells can be integrated in parallel, with no dependencies between the cells.
The parallelism strategy is based on this observation, with cell groups distributed between parallel processes.
That is, the basic unit of parallel work is a cell group, not individual cells, or a more fine-grained unit like individual mechanisms or nodes in cells.

Parallelism is implemented at two levels of granularity:
\begin{enumerate}
\item
    \textbf{MPI (course-grained)}: the cell groups are distributed between MPI ranks.
\item
    \textbf{Thread (fine-grained)}: each cell group on an MPI rank is assigned to a thread.
\end{enumerate}
Threading uses pthreads, with explicit communication of spike information between both threads and MPI processes.
Each cell group is stored in a \lst{NrnThread} data structure (see \fig{lst:NrnThreadInfo}), which is named as such because each cell group is assigned to a thread (the name could be changed to better represent the data).

\begin{note}
The fine fine-grained parallelism could be implemented on a finer scale than it currently is, i.e. with multiple threads processing each cell group/cell. There was a CUDA implementation of Neuron that took this approach, and on the GPU this is probably necessary to expose enough parallelism. There have been some successful tests on the Xeon Phi where each thread was assigned a cell group.
\end{note}
\begin{todo}
There may be features added to \neuron that will introduce tighter coupling between cells during time step integration. Get Francesco and Michael to help understand what these are, and what impact they will have.
\end{todo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cell Groups}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The cells in a cell group are ``flattened'' together in the \lst{NrnThread} data structure, i.e. individual cells are not stored seperately.
As an example, for the TEST2 dataset distributed with the PCP:
\begin{infobox}{Cell group statistics for the TEST2 data set}
\begin{itemize}[leftmargin=*]
\item
    There are around 60--70 cells per cell group.
\item
    Each cell has of the order 400--450 nodes.
\item
    Each cell group has 25,000--30,000 nodes in total.
\end{itemize}
\end{infobox}
    The cells are packed together with all node and mechanism data for all nodes in all cells in the cell group packed into flat arrays.
        Given $n_c$ cells in the cell group with a total of $n$ nodes, the root nodes are indexed \lst{[1:n_c]}, and the rest of the nodes are indexed \lst{[n_c+1:n]}. This is the reason that the loops in the tridiagonal solve are over \lst{child_nodes=ncells+1:nnondes} and not over \lst{child_nodes=1:nnodes} in \lstref{lst:solveminimal}.

%-----------------------------------------------------------------------------
\begin{note}
Storing the cells in the cell group as such means that all of the steps in a timestep in \fig{fig:timestepDiagram} are applied to the entire cell group, not to individual cells.
This ignores the fact that the cells can be integrated in time independently, with coordination only required when communicating spike events.
An obvious approach for introducing more fine-grained parallelism is to distriubute individual cells, not cell groups, between threads.
This approach exposes more work that can be performed in parallel, however, it has load balancing issues, because of the variation in complexity between indivdual cells.
\end{note}
%-----------------------------------------------------------------------------

The main storage requirements for a mechanism are storing the vectors that describe the linear system, and the vectors that describe state for each mechanism.
These vectors are not allocated separately, instead a single memory buffer is allocated (\lst{NrnThread::data[]}), and pointers for matrix vectors and mechanism state point to sub-arrays in \lst{NrnThread::data[]}
\footnote{
Allocating memory in one large block is more memory efficient than allocating many smaller vectors with individual calls to \lst{malloc}.
}.
This process of allocating the memory is performed in the function \lst{read_phase2} in \file{nrniv/nrn_setup.cpp}.

The layout of vectors in the \lst{data[]} array is shown in \fig{fig:dataLayout}.
When the cell group is read from file, the total size of the \lst{data[]} array is determined, then the memory allocated.
The vectors that describe the linear system and the area of each node, \lst{[VEC_RHS, VEC_D, VEC_A, VEC_B, VEC_V, actual_area]}, are placed at the front of the data vector.
The state data for the mechanisms are then packed at the end of the buffer.

%------------------------------------------------------------------
\begin{myfigure}{data layout for a cell group and mechanism}{fig:dataLayout}
\begin{center}
\includegraphics[width=\textwidth]{./images/dataStorage.pdf}
\end{center}
Layout of the ``flattened'' matrix and mechanism state data for a cell group, and the mechanism data structure that points into it. There are $m$ mechanisms, and there are $n$ nodes in the illustrated mechanism $M_1$.
\end{myfigure}
%------------------------------------------------------------------

In \neuron the mechanisms are stored in a linked list attached to the \lst{NrnThread}.
Each entry in the linked list has a \lst{data[]} pointer that points to the part of the cell group array that contains the mechanism's state information.
\fig{fig:dataLayout} shows a mechanism with 4 state variables per node, labelled $a,b,c,~\text{and}~d$.
The \lst{Memb_list} structure also stores \lst{nodecount}, the number of nodes in the cell group with the mechanism, and a set of indexes \lst{nodeindices} of those nodes.

\begin{infobox}{Important properties of \texttt{nodeindices} in \fig{fig:dataLayout}}
\begin{enumerate}[leftmargin=*]
\item
The indices are not contiguous.
\item
There are repeated indices (the index 4 is repeated twice). These are an artifact of mesh coarsening, when multiple nodes in a fine mesh that have the same mechanism are combined into one node.
\end{enumerate}
\end{infobox}

The properties of the \lst{nodeindices} present challenges to fine-grained parallelism, both vectorization and if multiple threads to evaluate a mechanism on the same cell group.
The main computational loops, see for example \lstref{lst:current} or \lstref{lst:state}, load values from the matrix vectors using the \lst{nodeindices} to describe the access pattern.
The first property of the array, non-contiguous values, means that these operations can't be vectorized.
The second property, repeated indices, can cause race conditions when the indices are used when writing.

One way to remove these issues would be to allocate additional state variables for each value that has to be indexed from the global state in this manner.
These buffers could be loaded and flushed before and after the mechanism is applied respectively.
The overhead of the scatter and gather phases would have to be amortized over the cost of applying the mechanism kernel, which might only work for computationally intensive kernels such as the \lst{current}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Ion Channels}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{todo}
The implementation of Ion channels is still a bit of a mystery to me.

The main issue is how mechanisms interact with ion channels. Currently an opaque indirect index () is used, whereby a mechanism accesses ion channel variables via the \lst{ppvar} index vector.

These indexes are hard-coded in the cell group data that is read in from \file{.dat} files.
\end{todo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Neuron DSL and \hoc Files}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Mechanisms in Neuron are described using a DSL, which is translated into C code from \hoc files.
\neuron has the C files translated from the \hoc files that used by HBPNeuron\footnote{This reduces the complexity of \neuron, decoupling \neuron from Neuron, which will make it easier modify how the mechanisms are defined.}.
The translated mechanism definitions are in \file{mech/cfiles}, with one mechanism per C source file.
Each file defines functions (like \lst{jacob}, \lst{current}, \lst{alloc}) and meta-data (such as the number of variables required to store a mechanism's state for a node in the tree).

\begin{infobox}{The mechanisms are stored in global two arrays:}
\begin{enumerate}[leftmargin=*]
\item
    The array \lst{memb_func[]} is an array of \lst{Memb_func} structs. For each mechanism there are function pointers to \lst{jacob}, \lst{current}, \lst{state} and other mechanism-specific functions, and other meta-data specific to the mechanism.
\item
    Each member of the array \lst{memb_list[]} is of the type \lst{Memb_list} has a pointer to the per-node data, and a list of all the nodes that the mechanism is defined for, as illustrated in \fig{fig:dataLayout}.
\end{enumerate}
\end{infobox}

The implementation of each mechanism exports a function \lst{<mechanism name>_reg} that calls the \lst{register_mech()} function, adding the mechanisms function callbacks for \lst{jacob} etc, along with meta-data into the global arrays \lst{memb_func/list[]}.
As such, the implementation of each mechanism is a ``black box'', with only the function for registering the function exported.

Simplified source code for the implementation of these features is shown in \lstref{lst:NrnThreadInfo} and \lstref{lst:register_mech}.

%------------------------------------------------------------------
\label{sec:cellgroupccode}
\filelisting[C++] {./code/storage.cpp}{cell group storge in a \texttt{NrnThread} struct}{lst:NrnThreadInfo}
\filelisting[C++] {./code/register_mech.cpp}{registering a mechanism}{lst:register_mech}
%------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Observations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Some thoughts about refactoring the code that provide some interesting topics for discussion.
\begin{infobox}{Suggestions for further investigation}
\begin{itemize}[leftmargin=*]
\item
    Splitting the cells into separate, thread-specific, data structures complicates the code. This feels like it was added at some point to facilitate threading. This could be removed, to store all cells on a node/numa-region/device in a single pool.
\item
    The AoS storage is inefficient:
    \begin{itemize}
    \item
        It doesn't vectorize (see \fig{fig:papisample}).
    \item
        Poor cache/bandwidth utilization for loops (such as the \lst{jacob} update) where only one or two data values in a mechanism are touched. For each 64 byte cache line loaded, only 8 bytes of 64 may be used.
    \end{itemize}
    An SoA storage would address both issues.
\item
    With SoA vectorization of most loops would still not be possible, because of the gather/scatter implicit in using the node indexes to read/write to the \lst{VEC_V} and \lst{VEC_RHS} vectors.
    \begin{itemize}
    \item
        Perform scatter before computing the current, then gather after.
    \end{itemize}
\item
    There are many opportunities to improve the interface between the runtime (solvers etc) and user-defined mechanisms.
    \begin{itemize}
    \item
        This model is well-suited to standard object-oriented design.
    \item
        Much of the meta-data that is currently passed as runtime parameters could be stored as type-information that could help the compiler optimize more aggressively.
    \item
        The current DSL could be kept, with the code generation given an overhaul.
    \end{itemize}

\end{itemize}
\end{infobox}
