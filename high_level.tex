In this section an will look at how the main time-stepping algorithm, which accounts for nearly all the time to solution, is implemented. However, before looking at the time step implementation, a short description of all of the main stages of the simulation is presented to put the time stepping code in context.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overview of Main}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The driver code is in \file{nrniv/main1.cpp}. From the breakdown of the wall time for the TEST2 data set in \tbl{tbl:wallmain} it is apparent that from a computational point of view, the only component of importance is the time stepping/sover portion of the program in \lst{BBS_netpar_solve}, which takes 99.8\% of time to solution.

Nevertheless, I will breifly describe the other steps performed in the main driver:
\begin{enumerate}
\item \lst{mk_mech}\\
The mechanisms are configured. A text file with a tuple for each mechanism (name, unique index, parameter count, type,  etc \dots) is scanned. This text file is generated when the cell group files are generated by HBPNeuron. This information provides a bridge between the runtime and the mechanisms implemented using the Neuron \hoc language.
\item \lst{mk_netcvode}\\
    Creates a new \lst{NetCvode} object (see \file{nrnoc/netcvod.h/cpp}). This sets up the priority queue use to send and deliver spiking events.
\item \lst{nrn_setup}\\
    Before calling \lst{nrn_setup}, the configuration file \file{files.dat} is read to see how many and which cells are to be loaded for simulation. The cells are assigned in a round-robin fashion between the MPI ranks. Then \lst{nrn_setup} is called with a list of cell ids, to load the cell data from disk.
\item \lst{BBS_netpar_mindelay} and \lst{mk_spikevec_buffer}\\
    The mindelay and spike buffer size are configured. All neuron cells can be integrated-in-time independently for the \emph{minimum network connection delay}, i.e. spikes do not have to be delivered in the interval in which they were generated. These interval boundaries are used as synchronization points.
\item \lst{BBS_netpar_solve} \\
    The time stepping code. The focus of this report.
\item \lst{output_spikes} \\
    Write spike information to disk.
\end{enumerate}

%-------------------------------------------------------------------------------
\begin{table}[htp!]
    \centering
%-------------------------------------------------------------------------------
\begin{tabular}{lrr}
\hline
section                    &    wall time (s) & contribution \% \\
\hline
\lst{mk_mech}            &    0.01   &    0.0\\
\lst{mk_netcvode}        &    0.00   &    0.0\\
\lst{nrn_setup}          &    0.69   &    0.2\\
mindelay/spike buffer      &    0.15   &    0.0\\
\lst{BBS_netpar_solve}   &    388.93 &   99.8\\
\lst{output_spikes}      &    0.01   &    0.0\\
\hline
\end{tabular}
%-------------------------------------------------------------------------------
\label{tbl:wallmain}
\caption{Breakdown of wall time for TEST2 data set running on one node of Piz Daint, with 1 cell per core.}
\end{table}
%-------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Drilling Down to The Time Step}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The time stepping and all computation associated with it are performed in the \lst{BBS_netpar_solve} routine. A backtrace of the call tree from \lst{BBS_netpar_solve} to \lst{nrn_fixed_step_thread}, where 100\% of the computation is performed, is shown in \fig{fig:bbsnetpar}. The exact role played by each of these routines is not important now: some of them , and others are wrappers for passing cell groups to a pthread to separate integration (see \sect{sec:data} for more information about cell groups and threads). It is the integration of individual cell groups inside the lowest level function, \lst{nrn_fixed_step_thread} that interests us.

\begin{figure}[htp!]
\centering
\includegraphics[width=\textwidth]{./images/bbs_netpar_solve.pdf}
\caption{backtrace to the main computational routine.}
\label{fig:bbsnetpar}
\end{figure}

Each MPI rank has a set of cells assigned to it in a round robin fashion during the initialization phase (in the call to \lst{nrn_setup} in \lst{main}).
The cells are packaged together into \emph{cell groups}, with one cell group per intput file. The selection of cells in a cell group is chosen when the input files are generated to improve load balancing (static load balancing).
For more information, see \sect{sec:data}.

%The cells are then assigned to a \lst{NrnThread} on each MPI rank. An abreviated definition of is given in \fig{lst:NrnThread}

%\begin{figure}
%\begin{shaded}
%\begin{lstlisting}
%struct NrnThread {
%  // list of mechanisms
%  NrnThreadMembList *tml;
%
%  int ncell;        // number of cells
%  int end;          // number of segments
%  double *_data;    // data for all segment values
%  ...               // other data fields
%
%  // arrays holding matrix system values
%  double *_actual_rhs;
%  double *_actual_d;
%  double *_actual_a;
%  ...
%  // area of each segment
%  double *_actual_area;
%  // parent index of segments
%  int *_v_parent_index;
%};
%\end{lstlisting}
%\end{shaded}
%\label{lst:NrnThread}
%\caption{The definition of the \lst{NrnThread} data type from \file{nrnoc/multicore.h}. Note that many data members have been removed, with just some fields of interest included.}
%\end{figure}

%*******************************************************************************
\begin{figure}[htp!]
\centering
\includegraphics[width=\textwidth]{./images/calltree.pdf}
\caption{Calltree and percentage of wall time contribution for the main computational algorithm. Branches marked in blue indicate a significant contribution to wall time.}
\label{fig:calltree}
\end{figure}
%*******************************************************************************

